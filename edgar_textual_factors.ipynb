{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e43bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe57446",
   "metadata": {},
   "source": [
    "# Textual factors\n",
    "\n",
    "Our first approach to process the 10-K reports data is to construct the textual factors.\n",
    "First, we create a dictionary that contains each document's term-frequency.\n",
    "Second, we create textual clusters using locality-sensitive hashing (LSH) with word2vec embeddings and a fast clustering technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43a5a1",
   "metadata": {},
   "source": [
    "## 1. Create term-frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0529fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edgar_w2v_200.txt\") as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    w2v_embeddings = {str(line.split()[0]): [float(x) for x in line.split()[1:]] for line in lines}\n",
    "\n",
    "w2v_vocab = w2v_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2510a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_tokenizer(corpus):\n",
    "    \"\"\"\n",
    "    Input: a corpus\n",
    "    Output: a list of tokens (strings)\n",
    "    \"\"\"\n",
    "    tokens = nlp(corpus.lower())\n",
    "    tokens = [str(token) for token in tokens]    \n",
    "    return tokens\n",
    "\n",
    "def tf_dict(corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: a corpus and a vocabulary (set or dictionary-like)\n",
    "    Output: a dictionary mapping token (if the token is in the dictionary) to the frequency of it appearing \n",
    "            in the corpus \n",
    "    \"\"\"\n",
    "    corpus_toks = spacy_tokenizer(corpus)\n",
    "    res = {}\n",
    "    for token in corpus_toks:\n",
    "        if token in vocab:\n",
    "            res[token] = res.get(token, 0) + 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497f66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in range(2010, 2020):\n",
    "#    for filename in os.listdir(\"./{}\".format(year)):\n",
    "#        temp = {}\n",
    "#        with open(\"./{}/{}\".format(year, filename), 'r') as f:\n",
    "#            corpus = json.load(f)\n",
    "#        tf_7A_w2v = tf_dict(corpus[\"section_7A\"], w2v_vocab) # only include terms that are in w2v_vocab\n",
    "#        temp = {\"year\": year, \"cik\": corpus[\"cik\"], \"section_7A\": tf_7A_w2v}\n",
    "#        with open(\"./tf/tf_w2v/{}_{}.json\".format(corpus[\"cik\"], str(year)), 'w') as outfile:\n",
    "#            json.dump(temp, outfile)\n",
    "#        tf_7A_glove = tf_dict(corpus[\"section_7A\"], glove_vocab) # only include terms that are in glove_vocab\n",
    "#        temp = {\"year\": year, \"cik\": corpus[\"cik\"], \"section_7A\": tf_7A_glove}\n",
    "#        with open(\"./tf/tf_glove/{}_{}.json\".format(corpus[\"cik\"], str(year)), 'w') as outfile:\n",
    "#            json.dump(temp, outfile)\n",
    "#    print(\"Year {} completed.\".format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f014fe",
   "metadata": {},
   "source": [
    "## 2. Clustering via LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b607fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cs_similarity(w, V):\n",
    "    \"\"\"\n",
    "    w: d by 1 array\n",
    "    V: n by d array\n",
    "    \n",
    "    return: n values of inner products Vw\n",
    "    \"\"\"\n",
    "    w = w.reshape(-1, 1)\n",
    "    d = w.shape[0]\n",
    "    V = V.reshape(-1, d)\n",
    "    l2norms = np.sqrt((V * V).sum(axis = 1)).reshape(-1, 1)\n",
    "    V = V / l2norms\n",
    "    w = w / np.sqrt(np.sum(w * w))\n",
    "    \n",
    "    res = np.dot(V, w)\n",
    "    \n",
    "    return res.squeeze()\n",
    "\n",
    "class RP:\n",
    "    \n",
    "    def __init__(self, nbits, d, emb, dup):\n",
    "        self.nbits = nbits\n",
    "        self.d = d\n",
    "        self.dup = dup\n",
    "        self.ids_not_visited = set(range(len(emb.keys())))\n",
    "        self.stop = False\n",
    "        self.cluster = []\n",
    "        counter = 0\n",
    "        \n",
    "        self.emb_arr = []\n",
    "        self.lookup = {}\n",
    "        self.RPs = []\n",
    "        for _ in range(dup):\n",
    "            index = faiss.IndexLSH(d, nbits)\n",
    "            index.rrot.init(random.randint(0, 2 ** 30))\n",
    "            self.RPs.append(index)\n",
    "        print(\"LSH initialized.\")\n",
    "            \n",
    "        counter = 0\n",
    "        for word in emb:\n",
    "            self.lookup[counter] = word\n",
    "            self.emb_arr.append(emb[word])\n",
    "            counter += 1\n",
    "            \n",
    "        self.emb_arr = np.array(self.emb_arr, dtype = \"float32\")\n",
    "        for i in range(dup):\n",
    "            self.RPs[i].add(self.emb_arr)\n",
    "        print(\"Embedding completed.\")\n",
    "        \n",
    "    def nn(self, q_vec, k):\n",
    "        cand_ids = set()\n",
    "        for i in range(self.dup):\n",
    "            _, I = self.RPs[i].search(q_vec, k)\n",
    "            I = set(I.tolist()[0])\n",
    "            cand_ids.update(I)\n",
    "        return cand_ids\n",
    "    \n",
    "    def nn_refined(self, q_vec, k, thres, in_clustering = False):\n",
    "        cand_ids = self.nn(q_vec, k)\n",
    "        if in_clustering:\n",
    "            cand_ids = [i for i in cand_ids if i in self.ids_not_visited]\n",
    "            \n",
    "        temp = cs_similarity(q_vec.reshape(-1, 1), self.emb_arr[cand_ids,:])\n",
    "        res_ids = (temp > thres).nonzero()[0].astype(\"int\")\n",
    "        return (np.array(cand_ids)[res_ids]).tolist()\n",
    "    \n",
    "    def seq_one_step_update(self, k, thres):\n",
    "        if self.stop:\n",
    "            print(\"All words visited\")\n",
    "            return\n",
    "        query = self.ids_not_visited.pop()\n",
    "        q_vec = self.emb_arr[query,:].reshape(1, self.d)\n",
    "        res_ids = self.nn_refined(q_vec, k, thres, in_clustering = True)\n",
    "        \n",
    "        self.cluster.append([self.lookup[i] for i in res_ids] + [self.lookup[query]])\n",
    "        self.ids_not_visited = self.ids_not_visited.difference(set(res_ids))\n",
    "        if len(self.ids_not_visited) == 0:\n",
    "            self.stop = True\n",
    "            \n",
    "    def seq_clustering(self, k, thres):\n",
    "        counter = 0\n",
    "        start_time = time.time()\n",
    "        while not self.stop:\n",
    "            self.seq_one_step_update(k, thres)\n",
    "            if counter > 99 and counter % 100 == 0:\n",
    "                elps_time = time.time() - start_time\n",
    "                print(f\"  {len(self.ids_not_visited)} words in queue; {elps_time:.2f} sec passed\")\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6111e7",
   "metadata": {},
   "source": [
    "Power calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4a8f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8934550065054789, 0.1175569941988367)\n"
     ]
    }
   ],
   "source": [
    "def sensitivity(cs_sim1, cs_sim2, nbits, dup):\n",
    "    ang1 = np.arccos(cs_sim1) * 180 / np.pi\n",
    "    ang2 = np.arccos(cs_sim2) * 180 / np.pi\n",
    "    p1 = (180 - ang1) / 180\n",
    "    p2 = (180 - ang2) / 180\n",
    "    \n",
    "    p1 = p1 ** nbits\n",
    "    p2 = p2 ** nbits\n",
    "    \n",
    "    p1 = 1 - ((1 - p1) ** dup)\n",
    "    p2 = 1 - ((1 - p2) ** dup)\n",
    "    \n",
    "    return p1, p2\n",
    "\n",
    "print(sensitivity(0.5, 0, 10, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6dd3c",
   "metadata": {},
   "source": [
    "### 2.1 LSH using W2V embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c787db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH initialized.\n",
      "Embedding completed.\n",
      "  96680 words in queue; 9.47 sec passed\n",
      "  91260 words in queue; 18.63 sec passed\n",
      "  85829 words in queue; 27.84 sec passed\n",
      "  82405 words in queue; 36.91 sec passed\n",
      "  78144 words in queue; 45.91 sec passed\n",
      "  74039 words in queue; 54.88 sec passed\n",
      "  71684 words in queue; 63.87 sec passed\n",
      "  68187 words in queue; 72.74 sec passed\n",
      "  63670 words in queue; 81.60 sec passed\n",
      "  59723 words in queue; 90.43 sec passed\n",
      "  57803 words in queue; 99.25 sec passed\n",
      "  56483 words in queue; 107.94 sec passed\n",
      "  53949 words in queue; 116.68 sec passed\n",
      "  51762 words in queue; 125.29 sec passed\n",
      "  49491 words in queue; 133.94 sec passed\n",
      "  47706 words in queue; 142.54 sec passed\n",
      "  46247 words in queue; 151.08 sec passed\n",
      "  43097 words in queue; 159.67 sec passed\n",
      "  42226 words in queue; 168.31 sec passed\n",
      "  40544 words in queue; 177.93 sec passed\n",
      "  39001 words in queue; 187.28 sec passed\n",
      "  34085 words in queue; 196.14 sec passed\n",
      "  26433 words in queue; 205.06 sec passed\n",
      "  16917 words in queue; 214.08 sec passed\n",
      "  12617 words in queue; 222.43 sec passed\n",
      "  11029 words in queue; 230.71 sec passed\n",
      "  9853 words in queue; 238.99 sec passed\n",
      "  8922 words in queue; 247.29 sec passed\n",
      "  8153 words in queue; 255.56 sec passed\n",
      "  7413 words in queue; 263.87 sec passed\n",
      "  6826 words in queue; 272.04 sec passed\n",
      "  6339 words in queue; 280.34 sec passed\n",
      "  5901 words in queue; 288.62 sec passed\n",
      "  5537 words in queue; 296.85 sec passed\n",
      "  5235 words in queue; 305.07 sec passed\n",
      "  4943 words in queue; 313.22 sec passed\n",
      "  4646 words in queue; 321.40 sec passed\n",
      "  4400 words in queue; 329.64 sec passed\n",
      "  4106 words in queue; 337.87 sec passed\n",
      "  3874 words in queue; 346.08 sec passed\n",
      "  3658 words in queue; 354.25 sec passed\n",
      "  3428 words in queue; 362.44 sec passed\n",
      "  3233 words in queue; 370.65 sec passed\n",
      "  3045 words in queue; 378.82 sec passed\n",
      "  2841 words in queue; 387.10 sec passed\n",
      "  2666 words in queue; 395.25 sec passed\n",
      "  2503 words in queue; 403.53 sec passed\n",
      "  2339 words in queue; 411.78 sec passed\n",
      "  2180 words in queue; 419.96 sec passed\n",
      "  2018 words in queue; 428.14 sec passed\n",
      "  1879 words in queue; 436.34 sec passed\n",
      "  1745 words in queue; 444.49 sec passed\n",
      "  1590 words in queue; 452.56 sec passed\n",
      "  1457 words in queue; 460.69 sec passed\n",
      "  1335 words in queue; 468.87 sec passed\n",
      "  1208 words in queue; 477.12 sec passed\n",
      "  1084 words in queue; 485.26 sec passed\n",
      "  956 words in queue; 493.29 sec passed\n",
      "  834 words in queue; 501.46 sec passed\n",
      "  716 words in queue; 509.57 sec passed\n",
      "  605 words in queue; 517.66 sec passed\n",
      "  496 words in queue; 525.74 sec passed\n",
      "  390 words in queue; 533.83 sec passed\n",
      "  286 words in queue; 541.92 sec passed\n",
      "  179 words in queue; 550.06 sec passed\n",
      "  77 words in queue; 558.13 sec passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/sq3gs4d53ps1z1t0xbhh6h2h0000gn/T/ipykernel_47407/1181217015.py:64: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  res_ids = (temp > thres).nonzero()[0].astype(\"int\")\n"
     ]
    }
   ],
   "source": [
    "w2v_RP = RP(10, 200, w2v_embeddings, 128)\n",
    "w2v_RP.seq_clustering(300, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab3745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6677\n",
      "query: healthcare\n",
      "['healthcare', 'reimbursement', 'givers', 'medicare', 'education', 'trend', 'hospitals', 'hospital', 'medicaid', 'student', 'living', 'surgical', 'educational', 'emergency', 'pharmacy', 'affordable', 'vision', 'nursing', 'schools', 'fitness', 'rehabilitation', 'beneficiaries', 'dialysis', 'outpatient', 'eye', 'containment', 'pharmacies', 'beauty', 'cosmetic', 'respiratory', 'dietary', 'inpatient', 'nutrition', 'workplace', 'funeral', 'behavioral', 'aids', 'aca', 'portability', 'child', 'settings', 'sleep', 'wellness', 'hospice', 'orthopedic', 'nurses', 'prescriptions', 'correctional', 'doctor', 'ambulatory', 'curriculum', 'pps', 'mental', 'infant', 'veterans', 'psychiatric', 'acuity', 'nurse', 'pbm', 'elderly', 'counseling', 'affordability', 'dentists', 'youth', 'ppo', 'illnesses', 'seniors', 'anesthesia', 'capitation', 'mma', 'hospitalization', 'instructional', 'capitated', 'advocacy', 'hygiene', 'ambulance', 'stays', 'pharmacists', 'drg', 'urology', 'charity', 'hcfa', 'enrollees', 'tenet', 'omnicare', 'k-12', 'humana', 'continuum', 'veterinarians', 'therapists', 'maternity', 'practitioner', 'ascs', 'urgent', 'orthodontic', 'homecare', 'tricare', 'teacher', 'vocational', 'wellpoint', 'medco', 'snfs', 'inmate', 'coaching', 'medi', 'dentistry', 'prescribers', 'advocates', 'dentist', 'vitas', 'cremation', 'o&p', 'newborn', 'optometrists', 'pbms', 'rehabilitative', 'ppos', 'prosthetic', 'heath', 'ophthalmologists', 'uncompensated', 'rehab', 'tpas', 'homeownership', 'subacute', 'ipas', 'restorative', 'schip', 'enteral', 'ltac', 'unitedhealthcare', 'aarp', 'rug', 'pharmerica', 'mcos', 'indigent', 'geriatric', 'caring', 'irfs', 'aides', 'post-65', 'bariatric', 'novacare', 'wellcare', 'eap', 'pmpm', 'telehealth', 'enrollee', 'orthotic', 'eyecare', 'hospitalists', 'anesthesiology', 'veterinarian', 'hospitalist', 'therapist', 'hospices', 'pre-65', 'chiropractic', 'childcare', 'copayments', 'medicare+choice', 'optometric', 'convalescent', 'aide', 'inpatients', 'pediatricians', 'champus', 'congregate', 'readmission', 'copayment', 'perioperative', 'obstetrical', 'wellbeing', 'care']\n",
      "--------------------\n",
      "query: economy\n",
      "['markets', 'prices', 'industry', 'economic', 'competitive', 'economy', 'declines', 'sector', 'marketplace', 'prevailing', 'macroeconomic', 'marketability', 'illiquidity', 'market-', '•market', 'market']\n",
      "--------------------\n",
      "query: competitors\n",
      "['services', 'customers', 'product', 'systems', 'technology', 'marketing', 'manufacturing', 'software', 'programs', 'retail', 'supply', 'technologies', 'competition', 'materials', 'applications', 'competitors', 'solutions', 'design', 'proprietary', 'suppliers', '®', 'clients', 'manufacturers', 'successfully', 'brand', 'manufacture', 'processes', 'margins', 'offerings', 'goods', 'devices', 'distributors', 'raw', 'platform', 'pharmaceutical', 'wholesale', 'produce', 'content', 'specialty', 'capabilities', 'users', 'consumers', 'vendors', 'traditional', 'acceptance', 'supplies', 'parts', 'device', 'technological', 'manufacturer', 'brands', 'merchandise', 'hardware', 'supplier', 'manufactured', 'drugs', 'channels', 'automotive', 'packaging', 'installation', 'growing', 'solution', 'tools', 'retailers', 'marketed', 'distributor', 'electronics', 'commercialize', 'introduced', 'introduction', 'rapidly', 'competing', 'shipments', 'chemicals', 'metal', 'platforms', 'producers', 'branded', 'manufactures', 'launch', 'specialized', 'pressures', 'diagnostic', 'designs', 'enhancements', 'demands', 'custom', 'advantages', 'innovative', 'generic', 'functionality', 'specifications', 'compounds', 'apparel', 'internationally', 'oem', 'licensees', 'accessories', 'bulk', 'introduce', 'laser', 'containers', 'therapies', 'plastic', 'mass', 'environments', 'oems', 'networking', 'complementary', 'resellers', 'patented', 'engines', 'customized', 'competitor', 'ingredients', 'producer', 'packages', 'engineered', 'memory', 'penetration', 'introductions', 'modules', 'battery', 'connectivity', 'circuits', 'globally', 'versions', 'aftermarket', 'innovations', 'portable', 'packaged', 'adapt', 'soft', 'processors', 'beverages', 'cleaning', 'domestically', 'disposable', 'formulations', 'wholesalers', 'coatings', 'assemblies', 'commercializing', 'marketers', 'appliances', 'desktop', 'imported', 'widespread', 'nutritional', 'introducing', 'ingredient', 'batteries', 'sensors', 'drives', 'chips', 'niche', 'modular', 'tailored', 'pumps', 'installations', 'wafers', 'bundled', 'phones', 'resin', 'procure', 'marketer', 'kits', 'labels', 'processor', 'fabrics', 'polymer', 'resistance', 'medicines', 'printers', 'commercialized', 'differentiate', 'flash', 'supplying', 'coating', 'appliance', 'newer', 'filtration', 'lasers', 'panels', 'resins', 'broaden', 'redesign', 'differentiated', 'valves', 'ics', 'additives', 'assays', 'fabric', 'intensely', 'entrants', 'competitively', 'filter', 'oils', 'semiconductors', 'fibers', 'paint', 'penetrate', 'filters', 'medications', 'consumables', 'polymers', 'geographies', 'handsets', 'companion', 'clothing', 'configurations', 'ceramic', 'foundries', 'cosmetics', 'technologically', 'implants', 'subsystems', 'dispensing', 'tires', 'reagents', 'concentrates', 'kit', 'launches', 'alloys', 'fabricated', 'foam', 'specification', 'printer', 'injectable', 'vinyl', 'feedstocks', 'handheld', 'modems', 'drink', 'bags', 'accessory', 'consumable', 'bottles', 'differentiation', 'molded', 'insulation', 'lenses', 'advancements', 'tubes', 'marketplaces', 'lens', 'flavor', 'fragrance', 'subassemblies', 'refrigeration', 'localized', 'peripherals', 'apps', 'scanners', 'durability', 'additive', 'bundle', 'connectors', 'plates', 'substrates', 'flavors', 'reagent', 'seeds', 'generations', 'microprocessor', 'fragrances', 'radios', 'microprocessors', 'cores', 'compressed', 'fiberglass', 'varieties', 'reusable', 'fertilizers', 'packs', 'firearms', 'electrodes', 'watches', 'cartridges', 'intermediates', 'blends', 'eyewear', 'bottled', 'lubricants', 'notebook', 'cans', 'cabinets', 'motorcycles', 'capacitors', 'consoles', 'garments', 'aerosol', 'catalysts', 'verticals', 'transdermal', 'dvds', 'bundling', 'rechargeable', 'inkjet', 'discs', 'paints', 'detectors', 'polyurethane', 'analyzers', 'noncompetitive', 'silicone', 'asics', 'lighter', 'disposables', 'cleaner', 'mainstream', 'rugged', 'allograft', 'disks', 'enclosures', 'styling', 'subsystem', 'gloves', 'cups', 'inks', 'niches', 'blades', 'chipsets', 'powders', 'finishes', 'traits', 'remanufactured', 'adapters', 'patches', 'penetrating', 'membranes', 'installers', 'refrigerants', 'robots', 'laminate', 'bundles', 'ovens', 'adapter', 'biomaterials', 'sealants', 'syringes', 'laminates', 'masks', 'refrigerant', 'liners', 'chipset', 'moissanite', 'resells', 'wearable', 'juices', 'powdered', 'dressings', 'recyclable', 'versatility', 'conditioners', 'carbonated', 'balls', 'gum', 'dispensers', 'headsets', 'mattresses', 'shingles', 'cassettes', 'decking', 'gels', 'functionalities', 'activewear', 'photomasks', 'teas', 'skincare', 'herbicides', 'filler', 'testers', 'dyes', 'lubrication', 'injectors', 'omnipod', 'binders', 'trademarked', 'microcontrollers', 'tabletop', 'halide', 'nonwoven', 'helmets', 'installer', 'fillers', 'candles', 'sunglasses', 'mediums', 'microcontroller', 'freezers', 'starches', 'toner', 'insecticides', 'enhancers', 'ruggedized', 'karaoke', 'foams', 'srams', 'pots', 'copolyesters', 'stains', 'pillows', 'preservatives', 'elastomer', 'slurries', 'videogame', 'preservative', 'applicators', 'reels', 'colorants', 'waxes', 'eyeglass', 'videogames', 'ultracapacitors', 'faucets', 'mirel', 'encapsulants', 'synthetics', 'flavorings', 'sbcs', 'cements', 'toners', 'houseware', 'tampons', 'products']\n",
      "--------------------\n",
      "query: prices\n",
      "['markets', 'prices', 'industry', 'economic', 'competitive', 'economy', 'declines', 'sector', 'marketplace', 'prevailing', 'macroeconomic', 'marketability', 'illiquidity', 'market-', '•market', 'market']\n",
      "--------------------\n",
      "query: investors\n",
      "['you', 'investor', 'advisors', 'institutional', 'purchaser', 'purchasers', 'analysts', 'sponsors', 'buyers', 'accredited', 'blackstone', 'fortress', 'warburg', 'bondholders', 'pincus', 'debtholders', 'h&q', 'mscp', 'investors']\n",
      "--------------------\n",
      "query: oil\n",
      "['energy', 'power', 'oil', 'natural', 'electric', 'fuel', 'generation', 'water', 'commodity', 'storage', 'transportation', 'utility', 'transmission', 'drilling', 'coal', 'pipeline', 'mining', 'field', 'plants', 'electricity', 'crude', 'proved', 'producing', 'emissions', 'petroleum', 'quantities', 'gulf', 'pipelines', 'gathering', 'transport', 'ethanol', 'carbon', 'minerals', 'copper', 'basin', 'aluminum', 'depletion', 'propane', 'refining', 'gasoline', 'refinery', 'fuels', 'fields', 'diesel', 'heating', 'midstream', 'refined', 'fired', 'mcf', 'underground', 'corn', 'acreage', 'liquids', 'ore', 'greenhouse', 'pipe', 'steam', 'barrels', 'compression', 'ngl', 'dioxide', 'ngls', 'sulfur', 'shale', 'gases', 'reservoir', 'nitrogen', 'wastewater', 'railroad', 'lng', 'wyoming', 'combustion', 'lg&e', 'uranium', 'fossil', 'pulp', 'co2', 'sugar', 'fertilizer', 'turbine', 'hub', 'nymex', 'grain', 'transported', 'sand', 'enron', 'bcf', 'nstar', 'intrastate', 'mmbtu', 'extraction', 'refineries', 'kwh', 'asphalt', 'onshore', 'fluids', 'cement', 'feedstock', 'trucking', 'hydrocarbons', 'hydroelectric', 'pg&e', 'biodiesel', 'geothermal', 'questar', 'oilfield', 'grains', 'bbl', 'coke', 'unregulated', 'condensate', 'petrochemical', 'sppc', 'boe', 'hydro', 'anadarko', 'cogeneration', 'deepwater', 'reservoirs', 'hydrocarbon', 'turbines', 'shippers', 'permian', 'chevron', 'transporting', 'hydrogen', 'compressor', 'methane', 'mmcf', 'ammonia', 'ppas', 'downstream', 'mwh', 'oneok', 'biomass', 'cattle', 'upstream', 'metallurgical', 'og&e', 'soybean', 'monongahela', 'pumping', 'workover', 'dte', 'e&p', 'panhandle', 'dd&a', 'distillers', 'sempra', 'nonregulated', 'mcfe', 'bcfe', 'peaking', 'imbalance', 'nep', 'basins', 'wellhead', 'transports', 'appalachian', 'wti', 'transco', 'nicor', 'arp', 'bbls', 'imbalances', 'ethylene', 'marcellus', 'sjg', 'compressors', 'dth', 'lpg', 'mcv', 'lime', 'uns', 'fractionation', 'interruptible', 'sdg&e', 'limestone', 'megawatt', 'potash', 'btu', 'volumetric', 'cng', 'pork', 'refiners', 'bakken', 'methanol', 'alumina', 'fueling', 'liquefaction', 'wpsc', 'boilers', 'exxonmobil', 'frac', 'alagasco', 'brine', 'liquefied', 'piping', 'kern', 'lignite', 'nonutility', 'enogex', 'renewables', 'appalachia', 'unconventional', 'molybdenum', 'whiting', 'tanker', 'residue', 'terminalling', 'rockies', 'williston', 'completions', 'baseload', 'mboe', 'petrochemicals', 'pgs', 'njng', 'mains', 'o&r', 'markwest', 't&d', 'cbm', 'psnc', 'distillate', 'hog', 'socalgas', 'propylene', 'qep', 'cnx', 'sigeco', 'eca', 'ethane', 'atp', 'sulphur', 'mbbls', 'cerc', 'wexpro', 'terminaling', 'anr', 'denbury', 'cecony', 'cig', 'farmout', 'haynesville', 'mmcfe', 'butane', 'proppant', 'caustic', 'xto', 'sour', 'neg', 'belvieu', 'elizabethtown', 'coalbed', 'piceance', 'brazos', 'nyiso', 'distiller', 'inlet', 'olefins', 'wattenberg', 'wgeservices', 'sng', 'gaseous', 'ldc', 'deliverability', 'transtexas', 'hugoton', 'helium', 'bitumen', 'rec', 'sonat', 'anhydrous', 'bgss', 'transwestern', 'therms', 'therm', 'nitrous', 'pellet', 'coking', 'gabon', 'maritech', 'mmcfd', 'mbbl', 'bromine', 'trunkline', 'encana', 'saltwater', 'tuscarora', 'proppants', 'shales', 'unleaded', 'syngas', 'mmbbls', 'eex', 'proliance', 'arkoma', 'ecopetrol', 'gatherers', 'waterflood', 'rutile', 'midcontinent', 'ldcs', 'ler', 'tgp', 'pgm', 'wpx', 'caverns', 'jonah', 'cavern', 'uinta', 'pgc', 'hydropower', 'mdth', 'algonquin', 'pnr', 'eor', 'highmount', 'biogas', 'syncrude', 'slaughter', 'mmbtus', 'ncng', 'pepl', 'ngd', 'epng', 'rng', 'giddings', 'transok', 'tgpl', 'fractionators', 'antrim', 'sjrg', 'bbtu', 'ngpl', 'transmix', 'waha', 'hilcorp', 'dekatherm', 'tetco', 'oil-', 'flared', 'liquified', 'gas']\n",
      "--------------------\n",
      "query: geopolitical\n",
      "['weather', 'seasonality', 'rising', 'shortages', 'unemployment', 'wage', 'recession', 'escalation', 'seen', 'inflationary', 'geopolitical', 'unprecedented', 'escalating', 'modestly', 'gdp', 'risen', 'deflation', 'escalated', 'devaluations', 'controllable', 'trended', 'optimism', 'lingering', 'lagging', 'deflationary', 'moderating', 'inflation']\n",
      "--------------------\n",
      "query: recession\n",
      "['weather', 'seasonality', 'rising', 'shortages', 'unemployment', 'wage', 'recession', 'escalation', 'seen', 'inflationary', 'geopolitical', 'unprecedented', 'escalating', 'modestly', 'gdp', 'risen', 'deflation', 'escalated', 'devaluations', 'controllable', 'trended', 'optimism', 'lingering', 'lagging', 'deflationary', 'moderating', 'inflation']\n",
      "--------------------\n",
      "query: wage\n",
      "['weather', 'seasonality', 'rising', 'shortages', 'unemployment', 'wage', 'recession', 'escalation', 'seen', 'inflationary', 'geopolitical', 'unprecedented', 'escalating', 'modestly', 'gdp', 'risen', 'deflation', 'escalated', 'devaluations', 'controllable', 'trended', 'optimism', 'lingering', 'lagging', 'deflationary', 'moderating', 'inflation']\n",
      "--------------------\n",
      "query: inflation\n",
      "['weather', 'seasonality', 'rising', 'shortages', 'unemployment', 'wage', 'recession', 'escalation', 'seen', 'inflationary', 'geopolitical', 'unprecedented', 'escalating', 'modestly', 'gdp', 'risen', 'deflation', 'escalated', 'devaluations', 'controllable', 'trended', 'optimism', 'lingering', 'lagging', 'deflationary', 'moderating', 'inflation']\n",
      "--------------------\n",
      "query: inventory\n",
      "['finished', 'realizable', 'chain', 'obsolete', 'moving', 'sourcing', 'obsolescence', 'lifo', 'scrap', 'invoice', 'fifo', 'spare', 'tooling', 'idle', 'markdowns', 'consignment', 'markdown', 'shrinkage', 'shrink', 'turns', 'costing', 'unmarketable', 'consigned', 'refurbished', 'overstocks', 'perishable', 'underutilized', 'rework', 'assortments', 'closeout', 'inventoried', 'outdated', 'stockpiles', 'spares', 'sku', 'unusable', 'lcm', 'picking', 'stocked', 'unprocessed', 'scrapped', 'unfinished', 'nrv', 'reorder', 'overstock', 'salability', 'unsaleable', 'stale', 'unsalable', 'overstocked', 'backorders', 'locom', 'packaway', 'inventory']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_RP.cluster))\n",
    "queries = [\"healthcare\", \"economy\", \"competitors\", \"prices\", \"investors\", \"oil\", \"geopolitical\",\n",
    "           \"recession\", \"wage\", \"inflation\", \"inventory\"]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"query: {q}\")\n",
    "    for i in range(len(w2v_RP.cluster)):\n",
    "        if q in w2v_RP.cluster[i]:\n",
    "            print(w2v_RP.cluster[i])\n",
    "            print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6f872",
   "metadata": {},
   "source": [
    "## 3. Compute textual factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a92212de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commission': 0, 'currency': 0, '1934': 0, 'exchanges': 0, 'exchange': 0}\n"
     ]
    }
   ],
   "source": [
    "pre_factors = {}\n",
    "\n",
    "for i in range(len(w2v_RP.cluster)):\n",
    "    pre_factors[i] = {}\n",
    "    for tok in w2v_RP.cluster[i]:\n",
    "        pre_factors[i][tok] = 0       \n",
    "        \n",
    "print(pre_factors[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a080dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 document processed.\n",
      "2000 document processed.\n",
      "3000 document processed.\n",
      "4000 document processed.\n",
      "5000 document processed.\n",
      "6000 document processed.\n",
      "7000 document processed.\n",
      "8000 document processed.\n",
      "9000 document processed.\n",
      "10000 document processed.\n",
      "11000 document processed.\n",
      "12000 document processed.\n",
      "13000 document processed.\n",
      "14000 document processed.\n",
      "15000 document processed.\n",
      "16000 document processed.\n",
      "17000 document processed.\n",
      "18000 document processed.\n",
      "19000 document processed.\n",
      "20000 document processed.\n",
      "21000 document processed.\n",
      "22000 document processed.\n",
      "23000 document processed.\n",
      "24000 document processed.\n",
      "25000 document processed.\n",
      "26000 document processed.\n",
      "27000 document processed.\n",
      "28000 document processed.\n",
      "29000 document processed.\n",
      "30000 document processed.\n",
      "31000 document processed.\n",
      "32000 document processed.\n",
      "33000 document processed.\n",
      "34000 document processed.\n",
      "35000 document processed.\n",
      "36000 document processed.\n",
      "37000 document processed.\n",
      "38000 document processed.\n",
      "39000 document processed.\n",
      "40000 document processed.\n",
      "41000 document processed.\n",
      "42000 document processed.\n",
      "43000 document processed.\n",
      "44000 document processed.\n",
      "45000 document processed.\n",
      "46000 document processed.\n",
      "47000 document processed.\n",
      "48000 document processed.\n",
      "49000 document processed.\n",
      "50000 document processed.\n",
      "51000 document processed.\n",
      "52000 document processed.\n",
      "53000 document processed.\n",
      "54000 document processed.\n",
      "55000 document processed.\n",
      "56000 document processed.\n",
      "57000 document processed.\n",
      "58000 document processed.\n",
      "59000 document processed.\n",
      "60000 document processed.\n",
      "61000 document processed.\n",
      "62000 document processed.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(\"./tf/tf_w2v\"):\n",
    "    yr = int(filename.split(\".\")[0].split(\"_\")[1])\n",
    "    \n",
    "    # only use 2010-2017 data to train textual factors\n",
    "    if yr > 2017:\n",
    "        continue    \n",
    "    with open(\"./tf/tf_w2v/{}\".format(filename), \"r\") as f:\n",
    "        dta = json.load(f)        \n",
    "    tf = dta[\"section_7A\"]\n",
    "    for tok in tf:\n",
    "        for i in range(len(w2v_RP.cluster)):\n",
    "            if tok in pre_factors[i]:\n",
    "                pre_factors[i][tok] += tf[tok]\n",
    "                break\n",
    "    count += 1\n",
    "    if count > 999 and count % 1000 == 0:\n",
    "        print(f\"{count} document processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2778e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_word_counts = []\n",
    "for i in range(len(w2v_RP.cluster)):\n",
    "    temp = 0\n",
    "    for tok in pre_factors[i]:\n",
    "        temp += pre_factors[i][tok]\n",
    "    factor_word_counts.append(temp)\n",
    "    \n",
    "factors = []\n",
    "null_factor_ids = []\n",
    "for i in range(len(w2v_RP.cluster)):\n",
    "    toks = w2v_RP.cluster[i]\n",
    "    temp = []\n",
    "    for tok in toks:\n",
    "        if factor_word_counts[i] == 0:\n",
    "            null_factor_ids.append(i)\n",
    "            temp.append(pre_factors[i][tok])\n",
    "        else:\n",
    "            temp.append(pre_factors[i][tok] / factor_word_counts[i])\n",
    "    factors.append(temp)\n",
    "    \n",
    "null_factor_ids = list(set(null_factor_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a3a7f",
   "metadata": {},
   "source": [
    "## 4. Compute factor loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34e85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"company_tickers.json\"\n",
    "cik_to_tickers_dict = {}\n",
    "\n",
    "f = open(file_name)\n",
    "file = json.load(f)\n",
    "for company in file.values():\n",
    "    if cik_to_tickers_dict.get(str(company[\"cik_str\"]), None):\n",
    "        cik_to_tickers_dict[str(company[\"cik_str\"])].append(company[\"ticker\"])\n",
    "    else:\n",
    "        cik_to_tickers_dict[str(company[\"cik_str\"])] = [company[\"ticker\"]]\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88d7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_span = {}\n",
    "\n",
    "for filename in os.listdir(\"./tf/tf_w2v\"):\n",
    "    temp = filename.split(\".\")[0]\n",
    "    temp = temp.split(\"_\")\n",
    "    cik_span[temp[0]] = cik_span.get(temp[0], []) + [int(temp[1])]\n",
    "    \n",
    "cik_span = [cik for cik, year in cik_span.items() if set(range(2010, 2020)).issubset(set(year))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6636b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_final = []\n",
    "\n",
    "for cik in cik_span:\n",
    "    if cik not in cik_to_tickers_dict:\n",
    "        continue\n",
    "    save = True\n",
    "    for year in range(2010, 2020):\n",
    "        filename = \"_\".join([cik, str(year)]) + \".json\"\n",
    "        with open(\"./tf/tf_w2v/{}\".format(filename), \"r\") as f:\n",
    "            temp = json.load(f)\n",
    "            temp = temp[\"section_7A\"]\n",
    "        if len(temp.keys()) < 30:\n",
    "            save = False\n",
    "            break\n",
    "    if save:\n",
    "        cik_final.append(cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1333cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_2norms = []\n",
    "\n",
    "for i in range(len(factors)):\n",
    "    factor_2norms.append(np.sum(np.array(factors[i]) ** 2))\n",
    "    \n",
    "\n",
    "def loadings(dt_vec, factor, toks, factor_2norm):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        dt_vec: doc-term vector (dictionary)\n",
    "        factor: textual factor (list)\n",
    "        toks: tokens corresponding to the factor support (list)\n",
    "        factor_2norms: L2 norm of the factor (scalar)\n",
    "    \n",
    "    Output:\n",
    "        factor loading (scalar)\n",
    "    \"\"\"\n",
    "    if factor_2norm == 0:\n",
    "        return 0\n",
    "    res = 0\n",
    "    for j in range(len(toks)):\n",
    "        tok = toks[j]\n",
    "        if tok not in dt_vec:\n",
    "            continue\n",
    "        res += dt_vec[tok] * factor[j]\n",
    "    return res / factor_2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e7ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ciks processed\n",
      "200 ciks processed\n",
      "300 ciks processed\n",
      "400 ciks processed\n",
      "500 ciks processed\n",
      "600 ciks processed\n",
      "700 ciks processed\n",
      "800 ciks processed\n",
      "900 ciks processed\n",
      "1000 ciks processed\n",
      "1100 ciks processed\n",
      "1200 ciks processed\n",
      "1300 ciks processed\n",
      "1400 ciks processed\n",
      "1500 ciks processed\n",
      "1600 ciks processed\n",
      "1700 ciks processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = []\n",
    "count = 0\n",
    "\n",
    "for cik in cik_final:\n",
    "    for year in range(2010, 2020):\n",
    "        temp = [cik, year, cik_to_tickers_dict[cik][0]]\n",
    "        filename = \"_\".join([cik, str(year)]) + \".json\"\n",
    "        with open(\"./tf/tf_w2v/{}\".format(filename), \"r\") as f:\n",
    "            doc_term_vec = json.load(f)[\"section_7A\"]\n",
    "        for i in range(len(factors)):\n",
    "            if factor_2norms[i] == 0:\n",
    "                temp.append(0)\n",
    "            else:\n",
    "                temp.append(loadings(doc_term_vec, factors[i], w2v_RP.cluster[i], factor_2norms[i]))\n",
    "        final_data.append(temp)\n",
    "    count += 1\n",
    "    if count > 99 and count % 100 == 0:\n",
    "        print(f\"{count} ciks processed\")\n",
    "\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb91192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_data)\n",
    "df.to_csv(\"./w2v_textual_factors.csv\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19312cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
